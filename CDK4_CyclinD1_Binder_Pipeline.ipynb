{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afb8250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:35:33.981972Z",
     "iopub.status.busy": "2026-02-04T01:35:33.981898Z",
     "iopub.status.idle": "2026-02-04T01:35:34.782448Z",
     "shell.execute_reply": "2026-02-04T01:35:34.781868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ /home/xuchengjie/miniconda3/envs/BindCraft/bin/python -m pip install biopython pandas numpy scipy matplotlib tmtools\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: biopython in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (1.86)\n",
      "Requirement already satisfied: pandas in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (1.15.2)\n",
      "Requirement already satisfied: matplotlib in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (3.10.8)\n",
      "Requirement already satisfied: tmtools in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from matplotlib) (3.3.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"workdir\": \"/home/xuchengjie/Program/BindCraft\",\n",
      "  \"outdir\": \"/home/xuchengjie/Program/BindCraft/outputs\",\n",
      "  \"inputs_dir\": \"/home/xuchengjie/Program/BindCraft/inputs\",\n",
      "  \"pdb_ids\": [\n",
      "    \"6P8F\",\n",
      "    \"6P8H\",\n",
      "    \"2W9Z\"\n",
      "  ],\n",
      "  \"chains\": {\n",
      "    \"6P8F\": {\n",
      "      \"CyclinD1\": \"A\",\n",
      "      \"CDK4\": \"B\",\n",
      "      \"p27\": \"C\"\n",
      "    },\n",
      "    \"6P8H\": {\n",
      "      \"CyclinD1\": \"A\",\n",
      "      \"CDK4\": \"B\",\n",
      "      \"p21\": \"C\"\n",
      "    },\n",
      "    \"2W9Z\": {\n",
      "      \"CyclinD1\": \"A\",\n",
      "      \"CDK4\": \"B\"\n",
      "    }\n",
      "  },\n",
      "  \"contact_cutoff\": 4.5,\n",
      "  \"binder_len\": 50,\n",
      "  \"n_designs\": 20,\n",
      "  \"bindcraft_trim_on_oom\": true,\n",
      "  \"trim_radius\": 8.0,\n",
      "  \"identity_max\": 0.2,\n",
      "  \"similarity_max\": 0.35,\n",
      "  \"tm_score_max\": 0.35,\n",
      "  \"dock_engine\": \"contact_proxy\",\n",
      "  \"decoys_fast\": 25,\n",
      "  \"decoys_final\": 200,\n",
      "  \"xla_mem_fraction\": 0.7,\n",
      "  \"xla_disable_command_buffer\": true,\n",
      "  \"xla_preallocate\": false,\n",
      "  \"max_fast_dock\": 200,\n",
      "  \"max_final_dock\": 100,\n",
      "  \"max_L\": 5,\n",
      "  \"max_LL\": 1,\n",
      "  \"L_target\": 0.08,\n",
      "  \"L_penalty_alpha\": 10.0,\n",
      "  \"seed_topk\": 10,\n",
      "  \"iters\": 50,\n",
      "  \"proposals_per_iter\": 20,\n",
      "  \"keep_global\": 100,\n",
      "  \"bindcraft_script\": \"/home/xuchengjie/Program/BindCraft/bindcraft.py\",\n",
      "  \"bindcraft_advanced\": \"/home/xuchengjie/Program/BindCraft/settings_advanced/smoke_2stage.json\",\n",
      "  \"bindcraft_filters\": \"/home/xuchengjie/Program/BindCraft/settings_filters/peptide_relaxed_filters.json\",\n",
      "  \"rosetta_bin\": \"\",\n",
      "  \"rosetta_db\": \"\",\n",
      "  \"needle_exe\": \"needle\",\n",
      "  \"tmalign_exe\": \"TM-align\",\n",
      "  \"pymol_exe\": \"pymol\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell A: Environment and dependencies\n",
    "import os, sys, json, gzip, shutil, subprocess, textwrap, math, random, itertools\n",
    "from pathlib import Path\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\"] + pkgs\n",
    "    print(\"+\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "# Required packages (pyrosetta is handled by conda env)\n",
    "pip_install([\"biopython\", \"pandas\", \"numpy\", \"scipy\", \"matplotlib\", \"tmtools\"])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from Bio.SeqUtils import seq1 as _seq1\n",
    "\n",
    "# Safe 3-letter to 1-letter conversion\n",
    "def three_to_one(resname):\n",
    "    try:\n",
    "        return _seq1(resname)\n",
    "    except Exception:\n",
    "        return 'X'\n",
    "\n",
    "# Simple command runner\n",
    "\n",
    "def run(cmd, env=None):\n",
    "    if isinstance(cmd, (list, tuple)):\n",
    "        printable = \" \".join(map(str, cmd))\n",
    "    else:\n",
    "        printable = cmd\n",
    "    print(\"+\", printable)\n",
    "    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed ({proc.returncode})\\nSTDOUT:\\n{proc.stdout}\\nSTDERR:\\n{proc.stderr}\")\n",
    "    return proc.stdout, proc.stderr\n",
    "\n",
    "# Tool availability checks\n",
    "\n",
    "def check_tool(name_or_path):\n",
    "    if os.path.isabs(name_or_path) or os.path.sep in name_or_path:\n",
    "        return os.path.exists(name_or_path)\n",
    "    return shutil.which(name_or_path) is not None\n",
    "\n",
    "# CONFIG\n",
    "CONFIG = {\n",
    "    \"workdir\": os.path.abspath(\".\"),\n",
    "    \"outdir\": os.path.abspath(\"outputs\"),\n",
    "    \"inputs_dir\": os.path.abspath(\"inputs\"),\n",
    "    \"pdb_ids\": [\"6P8F\", \"6P8H\", \"2W9Z\"],\n",
    "    \"chains\": {\n",
    "        \"6P8F\": {\"CyclinD1\": \"A\", \"CDK4\": \"B\", \"p27\": \"C\"},\n",
    "        \"6P8H\": {\"CyclinD1\": \"A\", \"CDK4\": \"B\", \"p21\": \"C\"},\n",
    "        \"2W9Z\": {\"CyclinD1\": \"A\", \"CDK4\": \"B\"},\n",
    "    },\n",
    "    \"contact_cutoff\": 4.5,\n",
    "    \"binder_len\": 50,\n",
    "    \"n_designs\": 20,\n",
    "    \"bindcraft_trim_on_oom\": True,\n",
    "    \"trim_radius\": 8.0,\n",
    "    \"identity_max\": 0.20,\n",
    "    \"similarity_max\": 0.35,\n",
    "    \"tm_score_max\": 0.35,\n",
    "    \"dock_engine\": \"contact_proxy\",  # \"rosetta_cli\" | \"pyrosetta\" | \"contact_proxy\"\n",
    "    \"decoys_fast\": 25,\n",
    "    \"decoys_final\": 200,\n",
    "    \"xla_mem_fraction\": 0.7,\n",
    "    \"xla_disable_command_buffer\": True,\n",
    "    \"xla_preallocate\": False,\n",
    "    \"max_fast_dock\": 200,\n",
    "    \"max_final_dock\": 100,\n",
    "    \"max_L\": 5,\n",
    "    \"max_LL\": 1,\n",
    "    \"L_target\": 0.08,\n",
    "    \"L_penalty_alpha\": 10.0,\n",
    "    \"seed_topk\": 10,\n",
    "    \"iters\": 50,\n",
    "    \"proposals_per_iter\": 20,\n",
    "    \"keep_global\": 100,\n",
    "    # BindCraft / Rosetta paths\n",
    "    \"bindcraft_script\": os.path.abspath(\"bindcraft.py\"),\n",
    "    \"bindcraft_advanced\": os.path.abspath(\"settings_advanced/smoke_2stage.json\"),\n",
    "    \"bindcraft_filters\": os.path.abspath(\"settings_filters/peptide_relaxed_filters.json\"),\n",
    "    \"rosetta_bin\": os.environ.get(\"ROSETTA_BIN\", \"\"),\n",
    "    \"rosetta_db\": os.environ.get(\"ROSETTA_DB\", \"\"),\n",
    "    \"needle_exe\": os.environ.get(\"NEEDLE_EXE\", \"needle\"),\n",
    "    \"tmalign_exe\": os.environ.get(\"TMALIGN_EXE\", \"TM-align\"),\n",
    "    \"pymol_exe\": os.environ.get(\"PYMOL_EXE\", \"pymol\"),\n",
    "}\n",
    "\n",
    "Path(CONFIG[\"outdir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"inputs_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "# JAX/XLA memory controls for BindCraft\n",
    "if CONFIG.get(\"xla_preallocate\") is False:\n",
    "    os.environ.setdefault(\"XLA_PYTHON_CLIENT_PREALLOCATE\", \"false\")\n",
    "if CONFIG.get(\"xla_mem_fraction\"):\n",
    "    os.environ.setdefault(\"XLA_CLIENT_MEM_FRACTION\", str(CONFIG[\"xla_mem_fraction\"]))\n",
    "if CONFIG.get(\"xla_disable_command_buffer\"):\n",
    "    flags = os.environ.get(\"XLA_FLAGS\", \"\")\n",
    "    if \"--xla_gpu_enable_command_buffer\" not in flags:\n",
    "        os.environ[\"XLA_FLAGS\"] = (flags + \" --xla_gpu_enable_command_buffer=\").strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca01e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:35:34.783450Z",
     "iopub.status.busy": "2026-02-04T01:35:34.783305Z",
     "iopub.status.idle": "2026-02-04T01:35:34.935786Z",
     "shell.execute_reply": "2026-02-04T01:35:34.935003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://files.rcsb.org/download/6P8F.pdb.gz\n",
      "Downloading https://files.rcsb.org/download/6P8H.pdb.gz\n",
      "Downloading https://files.rcsb.org/download/2W9Z.pdb.gz\n",
      "Cleaned: /home/xuchengjie/Program/BindCraft/outputs/clean/6p8f_ABC.pdb /home/xuchengjie/Program/BindCraft/outputs/clean/6p8h_ABC.pdb /home/xuchengjie/Program/BindCraft/outputs/clean/2w9z_AB.pdb\n"
     ]
    }
   ],
   "source": [
    "# Cell B: Download / read / clean PDB\n",
    "from Bio.PDB import PDBParser, PDBIO, Select\n",
    "import urllib.request\n",
    "\n",
    "INPUTS = Path(CONFIG[\"inputs_dir\"])\n",
    "OUTDIR = Path(CONFIG[\"outdir\"])\n",
    "CLEAN_DIR = OUTDIR / \"clean\"\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "class ProteinSelect(Select):\n",
    "    def __init__(self, keep_chains):\n",
    "        self.keep_chains = set(keep_chains)\n",
    "    def accept_chain(self, chain):\n",
    "        return chain.id in self.keep_chains\n",
    "    def accept_residue(self, residue):\n",
    "        # Keep only standard amino acids (no water/ligands)\n",
    "        hetflag = residue.id[0]\n",
    "        return hetflag == \" \"\n",
    "\n",
    "\n",
    "def download_pdb(pdb_id, dest_path):\n",
    "    url = f\"https://files.rcsb.org/download/{pdb_id}.pdb.gz\"\n",
    "    print(\"Downloading\", url)\n",
    "    gz_path = dest_path.with_suffix('.pdb.gz')\n",
    "    if not gz_path.exists():\n",
    "        urllib.request.urlretrieve(url, gz_path)\n",
    "    # gunzip\n",
    "    if not dest_path.exists():\n",
    "        with gzip.open(gz_path, 'rb') as fin, open(dest_path, 'wb') as fout:\n",
    "            fout.write(fin.read())\n",
    "    return dest_path\n",
    "\n",
    "\n",
    "def clean_pdb(pdb_path, out_path, keep_chains):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"struct\", pdb_path)\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(str(out_path), select=ProteinSelect(keep_chains))\n",
    "    return out_path\n",
    "\n",
    "# Download and clean\n",
    "pdb_6p8f = download_pdb(\"6P8F\", INPUTS / \"6P8F.pdb\")\n",
    "pdb_6p8h = download_pdb(\"6P8H\", INPUTS / \"6P8H.pdb\")\n",
    "pdb_2w9z = download_pdb(\"2W9Z\", INPUTS / \"2W9Z.pdb\")\n",
    "\n",
    "clean_6p8f = clean_pdb(pdb_6p8f, CLEAN_DIR / \"6p8f_ABC.pdb\", [\"A\",\"B\",\"C\"])\n",
    "clean_6p8h = clean_pdb(pdb_6p8h, CLEAN_DIR / \"6p8h_ABC.pdb\", [\"A\",\"B\",\"C\"])\n",
    "clean_2w9z = clean_pdb(pdb_2w9z, CLEAN_DIR / \"2w9z_AB.pdb\", [\"A\",\"B\"])\n",
    "\n",
    "print(\"Cleaned:\", clean_6p8f, clean_6p8h, clean_2w9z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a1b4e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:35:34.937376Z",
     "iopub.status.busy": "2026-02-04T01:35:34.937205Z",
     "iopub.status.idle": "2026-02-04T01:35:35.258851Z",
     "shell.execute_reply": "2026-02-04T01:35:35.258349Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuchengjie/miniconda3/envs/BindCraft/lib/python3.10/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6P8F RMSD A/B {'A': 4.035795839976786, 'B': 20.335865594136237} TM A/B None None\n",
      "6P8H RMSD A/B {'A': 6.26549914952364, 'B': 20.080579668259528} TM A/B None None\n",
      "Saved mapping JSONs in /home/xuchengjie/Program/BindCraft/outputs/aln\n"
     ]
    }
   ],
   "source": [
    "# Cell C: Structure alignment + residue mapping (6P8F/6P8H -> 2W9Z)\n",
    "from Bio.PDB import PDBParser, Superimposer\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "ALN_DIR = OUTDIR / \"aln\"\n",
    "ALN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "parser = PDBParser(QUIET=True)\n",
    "\n",
    "\n",
    "def chain_residues(structure, chain_id):\n",
    "    chain = structure[0][chain_id]\n",
    "    residues = [r for r in chain if r.id[0] == \" \"]\n",
    "    return residues\n",
    "\n",
    "\n",
    "def residue_to_str(res):\n",
    "    resseq = res.id[1]\n",
    "    icode = res.id[2].strip()\n",
    "    return f\"{resseq}{icode}\" if icode else str(resseq)\n",
    "\n",
    "\n",
    "def get_sequence_and_residues(structure, chain_id):\n",
    "    residues = chain_residues(structure, chain_id)\n",
    "    seq = []\n",
    "    res_ids = []\n",
    "    for r in residues:\n",
    "        try:\n",
    "            aa = three_to_one(r.get_resname())\n",
    "        except KeyError:\n",
    "            aa = \"X\"\n",
    "        seq.append(aa)\n",
    "        res_ids.append(residue_to_str(r))\n",
    "    return \"\".join(seq), res_ids, residues\n",
    "\n",
    "\n",
    "def align_and_map(struct_a, chain_a, struct_b, chain_b):\n",
    "    seq_a, ids_a, res_a = get_sequence_and_residues(struct_a, chain_a)\n",
    "    seq_b, ids_b, res_b = get_sequence_and_residues(struct_b, chain_b)\n",
    "\n",
    "    blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "    aln = pairwise2.align.globalds(seq_a, seq_b, blosum62, -10, -0.5, one_alignment_only=True)[0]\n",
    "    aln_a, aln_b = aln.seqA, aln.seqB\n",
    "\n",
    "    # Sequence-based mapping\n",
    "    mapping = {}\n",
    "    i = j = 0\n",
    "    for aa, bb in zip(aln_a, aln_b):\n",
    "        if aa != \"-\" and bb != \"-\":\n",
    "            mapping[ids_a[i]] = ids_b[j]\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif aa != \"-\" and bb == \"-\":\n",
    "            i += 1\n",
    "        elif aa == \"-\" and bb != \"-\":\n",
    "            j += 1\n",
    "\n",
    "    # Structural alignment (CA) for RMSD info\n",
    "    ca_a = [r[\"CA\"] for r in res_a if \"CA\" in r]\n",
    "    ca_b = [r[\"CA\"] for r in res_b if \"CA\" in r]\n",
    "    n = min(len(ca_a), len(ca_b))\n",
    "    sup = Superimposer()\n",
    "    sup.set_atoms(ca_a[:n], ca_b[:n])\n",
    "    rmsd = sup.rms\n",
    "\n",
    "    return mapping, rmsd\n",
    "\n",
    "\n",
    "def try_tmtools(struct_a, chain_a, struct_b, chain_b):\n",
    "    try:\n",
    "        from tmtools import tm_align\n",
    "    except Exception:\n",
    "        return None\n",
    "    # Extract CA coords and sequences\n",
    "    res_a = chain_residues(struct_a, chain_a)\n",
    "    res_b = chain_residues(struct_b, chain_b)\n",
    "    ca_a = np.array([r[\"CA\"].get_coord() for r in res_a if \"CA\" in r], dtype=float)\n",
    "    ca_b = np.array([r[\"CA\"].get_coord() for r in res_b if \"CA\" in r], dtype=float)\n",
    "    if len(ca_a) == 0 or len(ca_b) == 0:\n",
    "        return None\n",
    "    seq_a, _, _ = get_sequence_and_residues(struct_a, chain_a)\n",
    "    seq_b, _, _ = get_sequence_and_residues(struct_b, chain_b)\n",
    "    try:\n",
    "        out = tm_align(ca_a, ca_b, seq_a, seq_b)\n",
    "        return out.tm_score\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Load structures\n",
    "s_6p8f = parser.get_structure(\"6P8F\", clean_6p8f)\n",
    "s_6p8h = parser.get_structure(\"6P8H\", clean_6p8h)\n",
    "s_2w9z = parser.get_structure(\"2W9Z\", clean_2w9z)\n",
    "\n",
    "maps = {}\n",
    "for pdb_id, struct in [(\"6P8F\", s_6p8f), (\"6P8H\", s_6p8h)]:\n",
    "    mapping = {}\n",
    "    rmsd_info = {}\n",
    "    for ch in [\"A\", \"B\"]:\n",
    "        m, rmsd = align_and_map(struct, ch, s_2w9z, ch)\n",
    "        mapping[ch] = m\n",
    "        rmsd_info[ch] = rmsd\n",
    "    maps[pdb_id] = mapping\n",
    "    tm_a = try_tmtools(struct, \"A\", s_2w9z, \"A\")\n",
    "    tm_b = try_tmtools(struct, \"B\", s_2w9z, \"B\")\n",
    "    print(pdb_id, \"RMSD A/B\", rmsd_info, \"TM A/B\", tm_a, tm_b)\n",
    "\n",
    "# Save mappings\n",
    "with open(ALN_DIR / \"map_6p8f_to_2w9z.json\", \"w\") as f:\n",
    "    json.dump(maps[\"6P8F\"], f, indent=2)\n",
    "with open(ALN_DIR / \"map_6p8h_to_2w9z.json\", \"w\") as f:\n",
    "    json.dump(maps[\"6P8H\"], f, indent=2)\n",
    "\n",
    "print(\"Saved mapping JSONs in\", ALN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c4c6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:35:35.259852Z",
     "iopub.status.busy": "2026-02-04T01:35:35.259772Z",
     "iopub.status.idle": "2026-02-04T01:35:35.272553Z",
     "shell.execute_reply": "2026-02-04T01:35:35.272146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hotspot lists in /home/xuchengjie/Program/BindCraft/outputs/hotspots\n"
     ]
    }
   ],
   "source": [
    "# Cell D: Interaction site identification (p27/p21 separately)\n",
    "from Bio.PDB import NeighborSearch\n",
    "\n",
    "HOTSPOT_DIR = OUTDIR / \"hotspots\"\n",
    "HOTSPOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_contacts(structure, receptor_chains, ligand_chain, cutoff):\n",
    "    # Collect atoms\n",
    "    receptor_atoms = []\n",
    "    ligand_atoms = []\n",
    "    for ch in receptor_chains:\n",
    "        receptor_atoms += [a for a in structure[0][ch].get_atoms() if a.get_parent().id[0] == \" \"]\n",
    "    ligand_atoms += [a for a in structure[0][ligand_chain].get_atoms() if a.get_parent().id[0] == \" \"]\n",
    "\n",
    "    ns = NeighborSearch(receptor_atoms)\n",
    "    rec_residues = set()\n",
    "    lig_residues = set()\n",
    "\n",
    "    for atom in ligand_atoms:\n",
    "        close = ns.search(atom.coord, cutoff)\n",
    "        if close:\n",
    "            lig_residues.add(atom.get_parent())\n",
    "            for a2 in close:\n",
    "                rec_residues.add(a2.get_parent())\n",
    "\n",
    "    def res_to_tag(res):\n",
    "        ch = res.get_parent().id\n",
    "        resseq = res.id[1]\n",
    "        icode = res.id[2].strip()\n",
    "        tag = f\"{ch}{resseq}{icode}\" if icode else f\"{ch}{resseq}\"\n",
    "        return tag\n",
    "\n",
    "    rec_tags = sorted({res_to_tag(r) for r in rec_residues})\n",
    "    lig_tags = sorted({res_to_tag(r) for r in lig_residues})\n",
    "    return rec_tags, lig_tags\n",
    "\n",
    "# p27 contacts from 6P8F\n",
    "rec_hotspots_p27, lig_iface_p27 = get_contacts(s_6p8f, [\"A\", \"B\"], \"C\", CONFIG[\"contact_cutoff\"])\n",
    "\n",
    "(HOTSPOT_DIR / \"rec_hotspots_p27_6p8f.txt\").write_text(\"\\n\".join(rec_hotspots_p27))\n",
    "(HOTSPOT_DIR / \"lig_iface_p27_6p8f.txt\").write_text(\"\\n\".join(lig_iface_p27))\n",
    "\n",
    "# p21 contacts from 6P8H\n",
    "rec_hotspots_p21, lig_iface_p21 = get_contacts(s_6p8h, [\"A\", \"B\"], \"C\", CONFIG[\"contact_cutoff\"])\n",
    "\n",
    "(HOTSPOT_DIR / \"rec_hotspots_p21_6p8h.txt\").write_text(\"\\n\".join(rec_hotspots_p21))\n",
    "\n",
    "print(\"Saved hotspot lists in\", HOTSPOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3608c479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:35:35.273526Z",
     "iopub.status.busy": "2026-02-04T01:35:35.273446Z",
     "iopub.status.idle": "2026-02-04T01:35:35.276656Z",
     "shell.execute_reply": "2026-02-04T01:35:35.276309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped hotspots saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell E: Hotspot mapping to 2W9Z + union\n",
    "\n",
    "with open(ALN_DIR / \"map_6p8f_to_2w9z.json\") as f:\n",
    "    map_6p8f = json.load(f)\n",
    "with open(ALN_DIR / \"map_6p8h_to_2w9z.json\") as f:\n",
    "    map_6p8h = json.load(f)\n",
    "\n",
    "\n",
    "def map_hotspots(hotspots, mapping):\n",
    "    out = []\n",
    "    for tag in hotspots:\n",
    "        chain = tag[0]\n",
    "        resid = tag[1:]\n",
    "        if chain in mapping and resid in mapping[chain]:\n",
    "            mapped = mapping[chain][resid]\n",
    "            out.append(f\"{chain}{mapped}\")\n",
    "    return sorted(set(out))\n",
    "\n",
    "p27_mapped = map_hotspots(rec_hotspots_p27, map_6p8f)\n",
    "p21_mapped = map_hotspots(rec_hotspots_p21, map_6p8h)\n",
    "union_mapped = sorted(set(p27_mapped) | set(p21_mapped))\n",
    "\n",
    "(HOTSPOT_DIR / \"hotspots_p27_mapped_2w9z.txt\").write_text(\"\\n\".join(p27_mapped))\n",
    "(HOTSPOT_DIR / \"hotspots_p21_mapped_2w9z.txt\").write_text(\"\\n\".join(p21_mapped))\n",
    "(HOTSPOT_DIR / \"hotspots_union_2w9z.txt\").write_text(\"\\n\".join(union_mapped))\n",
    "\n",
    "print(\"Mapped hotspots saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb181a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:35:35.277614Z",
     "iopub.status.busy": "2026-02-04T01:35:35.277537Z",
     "iopub.status.idle": "2026-02-04T01:37:04.488276Z",
     "shell.execute_reply": "2026-02-04T01:37:04.487787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: binder_len < 60. BindCraft defaults may be less efficient for 50 aa.\n",
      "+ /home/xuchengjie/miniconda3/envs/BindCraft/bin/python /home/xuchengjie/Program/BindCraft/bindcraft.py --settings /home/xuchengjie/Program/BindCraft/outputs/bindcraft/bindcraft_settings.json --filters /home/xuchengjie/Program/BindCraft/settings_filters/peptide_relaxed_filters.json --advanced /home/xuchengjie/Program/BindCraft/settings_advanced/smoke_2stage.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: BindCraft OOM. Retrying with trimmed receptor.\n",
      "+ /home/xuchengjie/miniconda3/envs/BindCraft/bin/python /home/xuchengjie/Program/BindCraft/bindcraft.py --settings /home/xuchengjie/Program/BindCraft/outputs/bindcraft/bindcraft_settings_trimmed_r8.0.json --filters /home/xuchengjie/Program/BindCraft/settings_filters/peptide_relaxed_filters.json --advanced /home/xuchengjie/Program/BindCraft/settings_advanced/smoke_2stage.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BindCraft outputs: /home/xuchengjie/Program/BindCraft/outputs/bindcraft/designs.fasta /home/xuchengjie/Program/BindCraft/outputs/bindcraft/designs_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell F: Run BindCraft on 2W9Z to generate 50 aa binders\n",
    "BINDCRAFT_DIR = OUTDIR / \"bindcraft\"\n",
    "BINDCRAFT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def compress_ranges(nums):\n",
    "    nums = sorted(set(nums))\n",
    "    ranges = []\n",
    "    if not nums:\n",
    "        return ranges\n",
    "    start = prev = nums[0]\n",
    "    for n in nums[1:]:\n",
    "        if n == prev + 1:\n",
    "            prev = n\n",
    "        else:\n",
    "            ranges.append((start, prev))\n",
    "            start = prev = n\n",
    "    ranges.append((start, prev))\n",
    "    return ranges\n",
    "\n",
    "\n",
    "def hotspot_list_to_string(hotspot_tags):\n",
    "    # Convert like A123, A124, B210 to \"A123-124,B210\"\n",
    "    by_chain = {}\n",
    "    for tag in hotspot_tags:\n",
    "        ch = tag[0]\n",
    "        resid = tag[1:]\n",
    "        try:\n",
    "            num = int(''.join([c for c in resid if c.isdigit()]))\n",
    "        except Exception:\n",
    "            continue\n",
    "        by_chain.setdefault(ch, []).append(num)\n",
    "    parts = []\n",
    "    for ch, nums in by_chain.items():\n",
    "        for start, end in compress_ranges(nums):\n",
    "            if start == end:\n",
    "                parts.append(f\"{ch}{start}\")\n",
    "            else:\n",
    "                parts.append(f\"{ch}{start}-{end}\")\n",
    "    return \",\".join(parts)\n",
    "\n",
    "\n",
    "def trim_receptor_by_hotspots(pdb_path, hotspots_file, radius):\n",
    "    from Bio.PDB import PDBParser, NeighborSearch, PDBIO, Select\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    struct = parser.get_structure(\"rec\", pdb_path)\n",
    "    hotspot_tags = set(Path(hotspots_file).read_text().strip().split())\n",
    "\n",
    "    if not hotspot_tags:\n",
    "        print(\"WARNING: No hotspot tags found; using full receptor.\")\n",
    "        return pdb_path\n",
    "\n",
    "    hotspot_atoms = []\n",
    "    for ch in struct[0]:\n",
    "        for res in ch:\n",
    "            if res.id[0] != \" \":\n",
    "                continue\n",
    "            tag = f\"{ch.id}{res.id[1]}{res.id[2].strip()}\" if res.id[2].strip() else f\"{ch.id}{res.id[1]}\"\n",
    "            if tag in hotspot_tags:\n",
    "                hotspot_atoms.extend(list(res.get_atoms()))\n",
    "\n",
    "    if not hotspot_atoms:\n",
    "        print(\"WARNING: No hotspot atoms found; using full receptor.\")\n",
    "        return pdb_path\n",
    "\n",
    "    all_atoms = [a for a in struct.get_atoms() if a.get_parent().id[0] == \" \"]\n",
    "    ns = NeighborSearch(all_atoms)\n",
    "\n",
    "    keep_res = set()\n",
    "    for hatom in hotspot_atoms:\n",
    "        for a in ns.search(hatom.coord, radius):\n",
    "            keep_res.add(a.get_parent())\n",
    "\n",
    "    # always keep hotspot residues\n",
    "    for ch in struct[0]:\n",
    "        for res in ch:\n",
    "            if res.id[0] != \" \":\n",
    "                continue\n",
    "            tag = f\"{ch.id}{res.id[1]}{res.id[2].strip()}\" if res.id[2].strip() else f\"{ch.id}{res.id[1]}\"\n",
    "            if tag in hotspot_tags:\n",
    "                keep_res.add(res)\n",
    "\n",
    "    class KeepSelect(Select):\n",
    "        def accept_residue(self, residue):\n",
    "            return residue in keep_res\n",
    "\n",
    "    out_pdb = BINDCRAFT_DIR / f\"trimmed_receptor_r{radius}.pdb\"\n",
    "    io = PDBIO()\n",
    "    io.set_structure(struct)\n",
    "    io.save(str(out_pdb), select=KeepSelect())\n",
    "    return out_pdb\n",
    "\n",
    "\n",
    "def run_bindcraft(target_pdb, hotspots_file, binder_len, n_designs):\n",
    "    if not check_tool(CONFIG[\"bindcraft_script\"]):\n",
    "        print(\"WARNING: BindCraft script not found. Skipping binder generation.\")\n",
    "        (BINDCRAFT_DIR / \"designs.fasta\").write_text(\"\")\n",
    "        pd.DataFrame(columns=[\"design_id\", \"sequence\"]).to_csv(BINDCRAFT_DIR / \"designs_metadata.csv\", index=False)\n",
    "        return None\n",
    "\n",
    "    # Warning for short binder length\n",
    "    if binder_len < 60:\n",
    "        print(\"WARNING: binder_len < 60. BindCraft defaults may be less efficient for 50 aa.\")\n",
    "\n",
    "    design_path = BINDCRAFT_DIR / \"run\"\n",
    "    design_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    hotspot_tags = (Path(hotspots_file).read_text().strip().split())\n",
    "    hotspot_string = hotspot_list_to_string(hotspot_tags)\n",
    "\n",
    "    settings = {\n",
    "        \"design_path\": str(design_path),\n",
    "        \"binder_name\": \"CDK4_CyclinD1\",\n",
    "        \"starting_pdb\": str(target_pdb),\n",
    "        \"chains\": \"A,B\",\n",
    "        \"target_hotspot_residues\": hotspot_string,\n",
    "        \"lengths\": [binder_len, binder_len],\n",
    "        \"number_of_final_designs\": n_designs,\n",
    "    }\n",
    "\n",
    "    settings_path = BINDCRAFT_DIR / \"bindcraft_settings.json\"\n",
    "    settings_path.write_text(json.dumps(settings, indent=2))\n",
    "\n",
    "    cmd = [sys.executable, CONFIG[\"bindcraft_script\"],\n",
    "           \"--settings\", str(settings_path),\n",
    "           \"--filters\", CONFIG[\"bindcraft_filters\"],\n",
    "           \"--advanced\", CONFIG[\"bindcraft_advanced\"]]\n",
    "    try:\n",
    "        run(cmd)\n",
    "        return design_path\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e)\n",
    "        if CONFIG.get(\"bindcraft_trim_on_oom\") and (\"RESOURCE_EXHAUSTED\" in msg or \"out of memory\" in msg or \"CUDA_ERROR_OUT_OF_MEMORY\" in msg):\n",
    "            print(\"WARNING: BindCraft OOM. Retrying with trimmed receptor.\")\n",
    "            radii = [CONFIG[\"trim_radius\"], 10.0, 8.0, 6.0]\n",
    "            for radius in radii:\n",
    "                trimmed_pdb = trim_receptor_by_hotspots(target_pdb, hotspots_file, radius)\n",
    "                design_path_trim = BINDCRAFT_DIR / f\"run_trimmed_r{radius}\"\n",
    "                design_path_trim.mkdir(parents=True, exist_ok=True)\n",
    "                settings[\"design_path\"] = str(design_path_trim)\n",
    "                settings[\"starting_pdb\"] = str(trimmed_pdb)\n",
    "                settings_path_trim = BINDCRAFT_DIR / f\"bindcraft_settings_trimmed_r{radius}.json\"\n",
    "                settings_path_trim.write_text(json.dumps(settings, indent=2))\n",
    "                cmd = [sys.executable, CONFIG[\"bindcraft_script\"],\n",
    "                       \"--settings\", str(settings_path_trim),\n",
    "                       \"--filters\", CONFIG[\"bindcraft_filters\"],\n",
    "                       \"--advanced\", CONFIG[\"bindcraft_advanced\"]]\n",
    "                try:\n",
    "                    run(cmd)\n",
    "                    return design_path_trim\n",
    "                except RuntimeError as e2:\n",
    "                    msg2 = str(e2)\n",
    "                    if \"RESOURCE_EXHAUSTED\" in msg2 or \"out of memory\" in msg2 or \"CUDA_ERROR_OUT_OF_MEMORY\" in msg2:\n",
    "                        print(f\"OOM at trim radius {radius}. Trying smaller radius...\")\n",
    "                        continue\n",
    "                    raise\n",
    "        raise\n",
    "\n",
    "\n",
    "def collect_bindcraft_outputs(design_path):\n",
    "    # Try to assemble sequences and metadata\n",
    "    fasta_path = BINDCRAFT_DIR / \"designs.fasta\"\n",
    "    meta_path = BINDCRAFT_DIR / \"designs_metadata.csv\"\n",
    "\n",
    "    if design_path is None or not Path(design_path).exists():\n",
    "        fasta_path.write_text(\"\")\n",
    "        pd.DataFrame(columns=[\"design_id\", \"sequence\"]).to_csv(meta_path, index=False)\n",
    "        return fasta_path, meta_path, {}\n",
    "\n",
    "    pose_map = {}\n",
    "    # Search for PDBs in Accepted/Ranked and Accepted\n",
    "    for sub in [\"Accepted/Ranked\", \"Accepted\", \"MPNN\", \"Trajectory\", \"Trajectory/Relaxed\", \"Rejected\"]:\n",
    "        p = Path(design_path) / sub\n",
    "        if p.exists():\n",
    "            for pdb in p.glob(\"*.pdb\"):\n",
    "                stem = pdb.stem\n",
    "                pose_map.setdefault(stem, str(pdb))\n",
    "\n",
    "    sequences = []\n",
    "    # Prefer mpnn_design_stats.csv\n",
    "    csv_path = Path(design_path) / \"mpnn_design_stats.csv\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"Sequence\" in df.columns and not df.empty:\n",
    "            for _, row in df.iterrows():\n",
    "                sequences.append((row.get(\"Design\", f\"design_{len(sequences)+1}\"), row[\"Sequence\"]))\n",
    "        df.to_csv(meta_path, index=False)\n",
    "\n",
    "    if not sequences:\n",
    "        # Try to derive from PDBs (sequence extraction)\n",
    "        from Bio.PDB import PDBParser\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        for stem, pdb in pose_map.items():\n",
    "            try:\n",
    "                struct = parser.get_structure(stem, pdb)\n",
    "                # Infer binder chain as chain not in receptor A/B (fallback to shortest chain)\n",
    "                binder_chain = None\n",
    "                chain_lens = {}\n",
    "                for ch in struct[0]:\n",
    "                    clen = sum(1 for r in ch if r.id[0] == \" \")\n",
    "                    chain_lens[ch.id] = clen\n",
    "                    if ch.id not in (\"A\", \"B\"):\n",
    "                        binder_chain = ch.id\n",
    "                if binder_chain is None and chain_lens:\n",
    "                    binder_chain = min(chain_lens, key=chain_lens.get)\n",
    "                if binder_chain is None:\n",
    "                    continue\n",
    "                seq = []\n",
    "                for r in struct[0][binder_chain]:\n",
    "                    if r.id[0] != \" \":\n",
    "                        continue\n",
    "                    try:\n",
    "                        seq.append(three_to_one(r.get_resname()))\n",
    "                    except KeyError:\n",
    "                        seq.append(\"X\")\n",
    "                sequences.append((stem, \"\".join(seq)))\n",
    "            except Exception:\n",
    "                continue\n",
    "        pd.DataFrame(sequences, columns=[\"design_id\", \"sequence\"]).to_csv(meta_path, index=False)\n",
    "\n",
    "    # If still too few sequences, pull from other run directories (if any)\n",
    "    if len(sequences) < 5:\n",
    "        extra = {}\n",
    "        for alt in sorted(BINDCRAFT_DIR.glob(\"run*\")):\n",
    "            if alt == Path(design_path):\n",
    "                continue\n",
    "            # Search for PDBs in alternate run\n",
    "            for sub in [\"Accepted/Ranked\", \"Accepted\", \"MPNN\", \"Trajectory\", \"Trajectory/Relaxed\", \"Rejected\"]:\n",
    "                p = alt / sub\n",
    "                if p.exists():\n",
    "                    for pdb in p.glob(\"*.pdb\"):\n",
    "                        extra.setdefault(pdb.stem, str(pdb))\n",
    "        if extra:\n",
    "            # extend pose_map for docking downstream\n",
    "            for stem, pdb in extra.items():\n",
    "                pose_map.setdefault(stem, pdb)\n",
    "            from Bio.PDB import PDBParser\n",
    "            parser = PDBParser(QUIET=True)\n",
    "            for stem, pdb in extra.items():\n",
    "                if any(stem == s[0] for s in sequences):\n",
    "                    continue\n",
    "                try:\n",
    "                    struct = parser.get_structure(stem, pdb)\n",
    "                    chain_lens = {ch.id: sum(1 for r in ch if r.id[0] == \" \") for ch in struct[0]}\n",
    "                    binder_chain = min(chain_lens, key=chain_lens.get) if chain_lens else None\n",
    "                    if binder_chain is None:\n",
    "                        continue\n",
    "                    seq = []\n",
    "                    for r in struct[0][binder_chain]:\n",
    "                        if r.id[0] != \" \":\n",
    "                            continue\n",
    "                        try:\n",
    "                            seq.append(three_to_one(r.get_resname()))\n",
    "                        except KeyError:\n",
    "                            seq.append(\"X\")\n",
    "                    sequences.append((stem, \"\".join(seq)))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            pd.DataFrame(sequences, columns=[\"design_id\", \"sequence\"]).to_csv(meta_path, index=False)\n",
    "\n",
    "\n",
    "    # Write FASTA\n",
    "    with open(fasta_path, \"w\") as f:\n",
    "        for name, seq in sequences:\n",
    "            f.write(f\">{name}\\n{seq}\\n\")\n",
    "\n",
    "    return fasta_path, meta_path, pose_map\n",
    "\n",
    "# Run BindCraft\n",
    "bindcraft_design_path = run_bindcraft(clean_2w9z, HOTSPOT_DIR / \"hotspots_union_2w9z.txt\", CONFIG[\"binder_len\"], CONFIG[\"n_designs\"])\n",
    "\n",
    "fasta_path, meta_path, pose_map = collect_bindcraft_outputs(bindcraft_design_path)\n",
    "print(\"BindCraft outputs:\", fasta_path, meta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb2ee3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:04.489371Z",
     "iopub.status.busy": "2026-02-04T01:37:04.489270Z",
     "iopub.status.idle": "2026-02-04T01:37:06.391771Z",
     "shell.execute_reply": "2026-02-04T01:37:06.391072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 2 of 49\n"
     ]
    }
   ],
   "source": [
    "# Cell G: p27 low-similarity filter (sequence + structural)\n",
    "from Bio import pairwise2\n",
    "from Bio.Align import substitution_matrices\n",
    "\n",
    "FILTER_DIR = OUTDIR / \"filter\"\n",
    "FILTER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build p27 interface sequence from 6P8F chain C\n",
    "seq_p27 = []\n",
    "res_order = []\n",
    "for res in s_6p8f[0][\"C\"]:\n",
    "    if res.id[0] != \" \":\n",
    "        continue\n",
    "    tag = f\"C{res.id[1]}{res.id[2].strip()}\" if res.id[2].strip() else f\"C{res.id[1]}\"\n",
    "    if tag in lig_iface_p27:\n",
    "        try:\n",
    "            aa = three_to_one(res.get_resname())\n",
    "        except Exception:\n",
    "            aa = \"X\"\n",
    "        seq_p27.append(aa)\n",
    "        res_order.append(tag)\n",
    "\n",
    "p27_iface_seq = \"\".join(seq_p27)\n",
    "(FILTER_DIR / \"p27_iface.fasta\").write_text(f\">p27_iface\\n{p27_iface_seq}\\n\")\n",
    "\n",
    "\n",
    "def score_alignment(seq1, seq2):\n",
    "    # Global alignment with BLOSUM62\n",
    "    blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "    aln = pairwise2.align.globalds(seq1, seq2, blosum62, -10, -0.5, one_alignment_only=True)[0]\n",
    "    a, b = aln.seqA, aln.seqB\n",
    "    matches = 0\n",
    "    positives = 0\n",
    "    aligned = 0\n",
    "    for x, y in zip(a, b):\n",
    "        if x == \"-\" or y == \"-\":\n",
    "            continue\n",
    "        aligned += 1\n",
    "        if x == y:\n",
    "            matches += 1\n",
    "            positives += 1\n",
    "        else:\n",
    "            key = (x, y) if (x, y) in blosum62 else (y, x)\n",
    "            if key in blosum62 and blosum62[key] > 0:\n",
    "                positives += 1\n",
    "    identity = matches / aligned if aligned else 0.0\n",
    "    similarity = positives / aligned if aligned else 0.0\n",
    "    return identity, similarity\n",
    "\n",
    "\n",
    "def needle_score(seq1, seq2):\n",
    "    # Use EMBOSS needle if available\n",
    "    if not check_tool(CONFIG[\"needle_exe\"]):\n",
    "        return None\n",
    "    tmp_dir = FILTER_DIR / \"needle_tmp\"\n",
    "    tmp_dir.mkdir(exist_ok=True)\n",
    "    f1 = tmp_dir / \"seq1.fasta\"\n",
    "    f2 = tmp_dir / \"seq2.fasta\"\n",
    "    out = tmp_dir / \"needle.txt\"\n",
    "    f1.write_text(f\">s1\\n{seq1}\\n\")\n",
    "    f2.write_text(f\">s2\\n{seq2}\\n\")\n",
    "    cmd = [CONFIG[\"needle_exe\"], \"-asequence\", str(f1), \"-bsequence\", str(f2), \"-gapopen\", \"10\", \"-gapextend\", \"0.5\", \"-outfile\", str(out), \"-auto\"]\n",
    "    try:\n",
    "        run(cmd)\n",
    "    except Exception:\n",
    "        return None\n",
    "    # Parse identity/similarity\n",
    "    identity = similarity = None\n",
    "    for line in out.read_text().splitlines():\n",
    "        if line.strip().startswith(\"# Identity:\"):\n",
    "            pct = line.split(\"(\")[-1].split(\"%\")[-2]\n",
    "            identity = float(pct) / 100.0\n",
    "        if line.strip().startswith(\"# Similarity:\"):\n",
    "            pct = line.split(\"(\")[-1].split(\"%\")[-2]\n",
    "            similarity = float(pct) / 100.0\n",
    "    if identity is None or similarity is None:\n",
    "        return None\n",
    "    return identity, similarity\n",
    "\n",
    "\n",
    "def compute_tm_score(pdb1, pdb2):\n",
    "    # Try tmtools python first\n",
    "    try:\n",
    "        from tmtools import tm_align\n",
    "        from Bio.PDB import PDBParser\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        s1 = parser.get_structure(\"s1\", pdb1)\n",
    "        s2 = parser.get_structure(\"s2\", pdb2)\n",
    "        # use first chain of each\n",
    "        c1 = list(s1[0])[0]\n",
    "        c2 = list(s2[0])[0]\n",
    "        ca1 = np.array([r[\"CA\"].get_coord() for r in c1 if \"CA\" in r])\n",
    "        ca2 = np.array([r[\"CA\"].get_coord() for r in c2 if \"CA\" in r])\n",
    "        if len(ca1) == 0 or len(ca2) == 0:\n",
    "            return None\n",
    "        seq1 = \"\".join([three_to_one(r.get_resname()) for r in c1 if r.id[0] == \" \" and \"CA\" in r])\n",
    "        seq2 = \"\".join([three_to_one(r.get_resname()) for r in c2 if r.id[0] == \" \" and \"CA\" in r])\n",
    "        out = tm_align(ca1, ca2, seq1, seq2)\n",
    "        return out.tm_score\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Try external TM-align\n",
    "    if check_tool(CONFIG[\"tmalign_exe\"]):\n",
    "        cmd = [CONFIG[\"tmalign_exe\"], pdb1, pdb2]\n",
    "        try:\n",
    "            stdout, _ = run(cmd)\n",
    "            for line in stdout.splitlines():\n",
    "                if line.strip().startswith(\"TM-score=\"):\n",
    "                    try:\n",
    "                        return float(line.split(\"=\")[1].split()[0])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Prepare p27 interface structure for TM-score\n",
    "p27_iface_pdb = FILTER_DIR / \"p27_iface.pdb\"\n",
    "if not p27_iface_pdb.exists():\n",
    "    from Bio.PDB import PDBIO, Select\n",
    "    class InterfaceSelect(Select):\n",
    "        def accept_residue(self, residue):\n",
    "            if residue.id[0] != \" \":\n",
    "                return False\n",
    "            tag = f\"C{residue.id[1]}{residue.id[2].strip()}\" if residue.id[2].strip() else f\"C{residue.id[1]}\"\n",
    "            return tag in lig_iface_p27\n",
    "    io = PDBIO()\n",
    "    io.set_structure(s_6p8f)\n",
    "    io.save(str(p27_iface_pdb), select=InterfaceSelect())\n",
    "\n",
    "# Load binder sequences\n",
    "from Bio import SeqIO\n",
    "binders = list(SeqIO.parse(fasta_path, \"fasta\"))\n",
    "\n",
    "rows = []\n",
    "for rec in binders:\n",
    "    seq = str(rec.seq)\n",
    "    # needle if available\n",
    "    ns = needle_score(p27_iface_seq, seq)\n",
    "    if ns is not None:\n",
    "        identity, similarity = ns\n",
    "        source = \"needle\"\n",
    "    else:\n",
    "        identity, similarity = score_alignment(p27_iface_seq, seq)\n",
    "        source = \"biopython\"\n",
    "\n",
    "    tm_score = None\n",
    "    # Attempt TM-score if pose exists\n",
    "    if rec.id in pose_map:\n",
    "        tm_score = compute_tm_score(pose_map[rec.id], str(p27_iface_pdb))\n",
    "\n",
    "    pass_seq = (identity <= CONFIG[\"identity_max\"]) and (similarity <= CONFIG[\"similarity_max\"])\n",
    "    pass_tm = (tm_score is None) or (tm_score < CONFIG[\"tm_score_max\"])\n",
    "    passed = pass_seq and pass_tm\n",
    "\n",
    "    rows.append({\n",
    "        \"binder_id\": rec.id,\n",
    "        \"identity\": identity,\n",
    "        \"similarity\": similarity,\n",
    "        \"tm_score\": tm_score,\n",
    "        \"pass_seq\": pass_seq,\n",
    "        \"pass_tm\": pass_tm,\n",
    "        \"passed\": passed,\n",
    "        \"method\": source,\n",
    "        \"sequence\": seq,\n",
    "    })\n",
    "\n",
    "if not rows:\n",
    "    sim_df = pd.DataFrame(columns=[\"binder_id\",\"identity\",\"similarity\",\"tm_score\",\"pass_seq\",\"pass_tm\",\"passed\",\"method\",\"sequence\"])\n",
    "else:\n",
    "    sim_df = pd.DataFrame(rows)\n",
    "sim_df.to_csv(FILTER_DIR / \"similarity_metrics.csv\", index=False)\n",
    "\n",
    "kept = sim_df[sim_df[\"passed\"]]\n",
    "with open(FILTER_DIR / \"kept.fasta\", \"w\") as f:\n",
    "    for _, row in kept.iterrows():\n",
    "        f.write(f\">{row['binder_id']}\\n{row['sequence']}\\n\")\n",
    "\n",
    "print(\"Kept\", len(kept), \"of\", len(sim_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e17fa5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:06.392873Z",
     "iopub.status.busy": "2026-02-04T01:37:06.392792Z",
     "iopub.status.idle": "2026-02-04T01:37:06.452449Z",
     "shell.execute_reply": "2026-02-04T01:37:06.451988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast docking scores saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell H: Docking + binding energy scoring (fast)\n",
    "DOCK_DIR = OUTDIR / \"docking\"\n",
    "DOCK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "_pyrosetta_inited = False\n",
    "\n",
    "def ensure_pyrosetta():\n",
    "    global _pyrosetta_inited\n",
    "    if _pyrosetta_inited:\n",
    "        return\n",
    "    try:\n",
    "        import pyrosetta\n",
    "        pyrosetta.init('-mute all')\n",
    "        _pyrosetta_inited = True\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"PyRosetta init failed: {e}\")\n",
    "\n",
    "\n",
    "def rosetta_exec(name):\n",
    "    if CONFIG[\"rosetta_bin\"]:\n",
    "        candidate = Path(CONFIG[\"rosetta_bin\"]) / name\n",
    "        if candidate.exists():\n",
    "            return str(candidate)\n",
    "    if check_tool(name):\n",
    "        return name\n",
    "    return None\n",
    "\n",
    "\n",
    "def interface_score_pyrosetta(pose, partners):\n",
    "    from pyrosetta import rosetta\n",
    "    # If still centroid, use a centroid score proxy\n",
    "    if not pose.is_fullatom():\n",
    "        scorefxn = rosetta.core.scoring.ScoreFunctionFactory.create_score_function(\"score3\")\n",
    "        return scorefxn(pose)\n",
    "    iam = rosetta.protocols.analysis.InterfaceAnalyzerMover(partners)\n",
    "    iam.apply(pose)\n",
    "    return iam.get_interface_dG()\n",
    "\n",
    "\n",
    "def ensure_fullatom(pose):\n",
    "    from pyrosetta import rosetta\n",
    "    if not pose.is_fullatom():\n",
    "        to_fa = rosetta.protocols.simple_moves.SwitchResidueTypeSetMover(\"fa_standard\")\n",
    "        to_fa.apply(pose)\n",
    "    return pose\n",
    "\n",
    "\n",
    "def infer_binder_chain_id(pose, receptor_chains=(\"A\",\"B\")):\n",
    "    chain_lengths = {}\n",
    "    for i in range(1, pose.num_chains()+1):\n",
    "        ch = pose.pdb_info().chain(pose.chain_begin(i))\n",
    "        chain_lengths[ch] = pose.chain_end(i) - pose.chain_begin(i) + 1\n",
    "        if ch not in receptor_chains:\n",
    "            return ch\n",
    "    if chain_lengths:\n",
    "        return min(chain_lengths, key=chain_lengths.get)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_partners(pose, binder_chain):\n",
    "    rec = []\n",
    "    for i in range(1, pose.num_chains()+1):\n",
    "        ch = pose.pdb_info().chain(pose.chain_begin(i))\n",
    "        if ch != binder_chain:\n",
    "            rec.append(ch)\n",
    "    return \"\".join(rec) + \"_\" + binder_chain\n",
    "\n",
    "\n",
    "def contact_score(pdb_path, receptor_chains=(\"A\",\"B\"), cutoff=4.5):\n",
    "    from Bio.PDB import PDBParser, NeighborSearch\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    struct = parser.get_structure(\"dock\", pdb_path)\n",
    "    # infer binder chain by shortest if needed\n",
    "    chain_lens = {ch.id: sum(1 for r in ch if r.id[0]==\" \") for ch in struct[0]}\n",
    "    binder_chain = None\n",
    "    for ch in struct[0]:\n",
    "        if ch.id not in receptor_chains:\n",
    "            binder_chain = ch.id\n",
    "            break\n",
    "    if binder_chain is None and chain_lens:\n",
    "        binder_chain = min(chain_lens, key=chain_lens.get)\n",
    "    if binder_chain is None:\n",
    "        return None\n",
    "    rec_atoms = []\n",
    "    for ch in receptor_chains:\n",
    "        if ch in struct[0]:\n",
    "            rec_atoms += [a for a in struct[0][ch].get_atoms() if a.get_parent().id[0] == \" \"]\n",
    "    lig_atoms = [a for a in struct[0][binder_chain].get_atoms() if a.get_parent().id[0] == \" \"]\n",
    "    if not rec_atoms or not lig_atoms:\n",
    "        return None\n",
    "    ns = NeighborSearch(rec_atoms)\n",
    "    count = 0\n",
    "    for a in lig_atoms:\n",
    "        if ns.search(a.coord, cutoff):\n",
    "            count += 1\n",
    "    return -float(count)\n",
    "\n",
    "\n",
    "def dock_pose_pyrosetta(pose, partners, nstruct):\n",
    "    from pyrosetta import rosetta\n",
    "    from pyrosetta.rosetta.protocols.docking import DockingProtocol, setup_foldtree\n",
    "    from pyrosetta.rosetta.utility import vector1_bool\n",
    "\n",
    "    # Setup foldtree\n",
    "    try:\n",
    "        v = vector1_bool()\n",
    "        v.append(True)\n",
    "        setup_foldtree(pose, partners, v)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dock = DockingProtocol()\n",
    "    dock.set_partners(partners)\n",
    "\n",
    "    best_score = None\n",
    "    best_pose = None\n",
    "    for _ in range(nstruct):\n",
    "        p = pose.clone()\n",
    "        ensure_fullatom(p)\n",
    "        if p.is_fullatom():\n",
    "            try:\n",
    "                dock.apply(p)\n",
    "            except Exception:\n",
    "                # fallback: no docking move\n",
    "                pass\n",
    "        sc = interface_score_pyrosetta(p, partners)\n",
    "        if best_score is None or sc < best_score:\n",
    "            best_score = sc\n",
    "            best_pose = p.clone()\n",
    "    return best_score, best_pose\n",
    "\n",
    "\n",
    "# Load kept binders\n",
    "kept_fasta = FILTER_DIR / \"kept.fasta\"\n",
    "kept_binders = list(SeqIO.parse(kept_fasta, \"fasta\"))\n",
    "\n",
    "rows = []\n",
    "for rec in kept_binders[:CONFIG[\"max_fast_dock\"]]:\n",
    "    if rec.id not in pose_map:\n",
    "        continue\n",
    "    if CONFIG[\"dock_engine\"] == \"contact_proxy\":\n",
    "        score = contact_score(pose_map[rec.id], cutoff=CONFIG[\"contact_cutoff\"])\n",
    "        best_pose_path = pose_map[rec.id]\n",
    "    elif CONFIG[\"dock_engine\"] == \"rosetta_cli\":\n",
    "        # placeholder for external Rosetta if installed\n",
    "        score = None\n",
    "        best_pose_path = pose_map[rec.id]\n",
    "    else:\n",
    "        ensure_pyrosetta()\n",
    "        import pyrosetta\n",
    "        pose = pyrosetta.pose_from_pdb(pose_map[rec.id])\n",
    "        ensure_fullatom(pose)\n",
    "        binder_chain = infer_binder_chain_id(pose)\n",
    "        if binder_chain is None:\n",
    "            continue\n",
    "        partners = get_partners(pose, binder_chain)\n",
    "        score, best_pose = dock_pose_pyrosetta(pose, partners, CONFIG[\"decoys_fast\"])\n",
    "        best_pose_path = str(DOCK_DIR / f\"{rec.id}_best_fast.pdb\")\n",
    "        if best_pose is not None:\n",
    "            best_pose.dump_pdb(best_pose_path)\n",
    "    rows.append({\"binder_id\": rec.id, \"docking_score\": score, \"best_pose_path\": best_pose_path})\n",
    "\n",
    "if not rows:\n",
    "    fast_df = pd.DataFrame(columns=[\"binder_id\",\"docking_score\",\"best_pose_path\"])\n",
    "else:\n",
    "    fast_df = pd.DataFrame(rows)\n",
    "fast_df.to_csv(DOCK_DIR / \"docking_scores_fast.csv\", index=False)\n",
    "print(\"Fast docking scores saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779fd92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:06.453627Z",
     "iopub.status.busy": "2026-02-04T01:37:06.453545Z",
     "iopub.status.idle": "2026-02-04T01:37:06.640596Z",
     "shell.execute_reply": "2026-02-04T01:37:06.640093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation outputs saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell I: Docking score vs sequence features correlation\n",
    "ANALYSIS_DIR = OUTDIR / \"analysis\"\n",
    "ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load sequences and scores\n",
    "try:\n",
    "    fast_df = pd.read_csv(DOCK_DIR / \"docking_scores_fast.csv\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    fast_df = pd.DataFrame(columns=[\"binder_id\", \"docking_score\", \"best_pose_path\"])\n",
    "\n",
    "seq_df = sim_df[[\"binder_id\", \"sequence\"]].drop_duplicates()\n",
    "merged = fast_df.merge(seq_df, on=\"binder_id\", how=\"left\")\n",
    "\n",
    "KD = {\n",
    "    'A':1.8,'C':2.5,'D':-3.5,'E':-3.5,'F':2.8,'G':-0.4,'H':-3.2,'I':4.5,\n",
    "    'K':-3.9,'L':3.8,'M':1.9,'N':-3.5,'P':-1.6,'Q':-3.5,'R':-4.5,'S':-0.8,\n",
    "    'T':-0.7,'V':4.2,'W':-0.9,'Y':-1.3\n",
    "}\n",
    "\n",
    "\n",
    "def seq_features(seq):\n",
    "    seq = seq or \"\"\n",
    "    n = len(seq) if seq else 1\n",
    "    count_L = seq.count(\"L\")\n",
    "    count_LL = sum(1 for i in range(len(seq)-1) if seq[i:i+2] == \"LL\")\n",
    "    frac_L = count_L / n\n",
    "    hyd = sum(KD.get(a, 0.0) for a in seq) / n\n",
    "    charge = (seq.count('K') + seq.count('R') + 0.1*seq.count('H')) - (seq.count('D') + seq.count('E'))\n",
    "    frac_G = seq.count('G') / n\n",
    "    frac_P = seq.count('P') / n\n",
    "    frac_aromatic = (seq.count('F') + seq.count('W') + seq.count('Y')) / n\n",
    "    return {\n",
    "        \"frac_L\": frac_L,\n",
    "        \"count_L\": count_L,\n",
    "        \"count_LL\": count_LL,\n",
    "        \"hydropathy\": hyd,\n",
    "        \"net_charge\": charge,\n",
    "        \"frac_G\": frac_G,\n",
    "        \"frac_P\": frac_P,\n",
    "        \"frac_aromatic\": frac_aromatic,\n",
    "    }\n",
    "\n",
    "if fast_df.empty:\n",
    "    # Create empty outputs + placeholder plots\n",
    "    pd.DataFrame().to_csv(ANALYSIS_DIR / \"pearson_corr.csv\")\n",
    "    pd.DataFrame().to_csv(ANALYSIS_DIR / \"spearman_corr.csv\")\n",
    "    empty_feat = pd.DataFrame(columns=[\"binder_id\",\"docking_score\",\"frac_L\",\"count_L\",\"count_LL\",\"hydropathy\",\"net_charge\",\"frac_G\",\"frac_P\",\"frac_aromatic\"])\n",
    "    empty_feat.to_csv(ANALYSIS_DIR / \"seq_score_correlations.csv\", index=False)\n",
    "\n",
    "    for name in [\"docking_vs_fracL.png\", \"feature_correlation_heatmap.png\"]:\n",
    "        plt.figure(figsize=(4,3))\n",
    "        plt.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ANALYSIS_DIR / name, dpi=200)\n",
    "        plt.close()\n",
    "    print(\"No docking scores; correlation analysis skipped.\")\n",
    "else:\n",
    "    feature_rows = []\n",
    "    for _, row in merged.iterrows():\n",
    "        feats = seq_features(row[\"sequence\"])\n",
    "        feats.update({\"binder_id\": row[\"binder_id\"], \"docking_score\": row[\"docking_score\"]})\n",
    "        feature_rows.append(feats)\n",
    "\n",
    "    feat_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "    # Correlations (numeric only)\n",
    "    num_df = feat_df.select_dtypes(include=[np.number])\n",
    "    pearson = num_df.corr(method='pearson')\n",
    "    spearman = num_df.corr(method='spearman')\n",
    "    pearson.to_csv(ANALYSIS_DIR / \"pearson_corr.csv\")\n",
    "    spearman.to_csv(ANALYSIS_DIR / \"spearman_corr.csv\")\n",
    "\n",
    "    # Plot docking_score vs frac_L\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.scatter(feat_df[\"frac_L\"], feat_df[\"docking_score\"], s=20)\n",
    "    plt.xlabel(\"frac_L\")\n",
    "    plt.ylabel(\"docking_score\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / \"docking_vs_fracL.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(spearman.values, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.xticks(range(len(spearman.columns)), spearman.columns, rotation=90, fontsize=8)\n",
    "    plt.yticks(range(len(spearman.index)), spearman.index, fontsize=8)\n",
    "    plt.colorbar(label='Spearman')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / \"feature_correlation_heatmap.png\", dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    feat_df.to_csv(ANALYSIS_DIR / \"seq_score_correlations.csv\", index=False)\n",
    "    print(\"Correlation outputs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56a9167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:06.641698Z",
     "iopub.status.busy": "2026-02-04T01:37:06.641604Z",
     "iopub.status.idle": "2026-02-04T01:37:06.646279Z",
     "shell.execute_reply": "2026-02-04T01:37:06.645928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 2 seed binders\n"
     ]
    }
   ],
   "source": [
    "# Cell J: Leu constraints + effective_score\n",
    "\n",
    "try:\n",
    "    feat_df = pd.read_csv(ANALYSIS_DIR / \"seq_score_correlations.csv\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    feat_df = pd.DataFrame(columns=[\"binder_id\",\"docking_score\",\"frac_L\",\"count_L\",\"count_LL\",\"hydropathy\",\"net_charge\",\"frac_G\",\"frac_P\",\"frac_aromatic\"])\n",
    "\n",
    "if feat_df.empty:\n",
    "    seed_top = pd.DataFrame(columns=[\"binder_id\",\"docking_score\",\"effective_score\"])\n",
    "else:\n",
    "    # Hard constraints\n",
    "    valid = feat_df[(feat_df[\"count_L\"] <= CONFIG[\"max_L\"]) & (feat_df[\"count_LL\"] <= CONFIG[\"max_LL\"])].copy()\n",
    "\n",
    "    # Effective score with soft penalty\n",
    "    valid[\"effective_score\"] = valid[\"docking_score\"] + CONFIG[\"L_penalty_alpha\"] * np.maximum(0, valid[\"frac_L\"] - CONFIG[\"L_target\"])\n",
    "\n",
    "    valid = valid.sort_values(\"effective_score\")\n",
    "    seed_top = valid.head(CONFIG[\"seed_topk\"]).copy()\n",
    "\n",
    "seed_top.to_csv(ANALYSIS_DIR / \"seed_top10.csv\", index=False)\n",
    "print(\"Selected\", len(seed_top), \"seed binders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce642d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:06.647282Z",
     "iopub.status.busy": "2026-02-04T01:37:06.647195Z",
     "iopub.status.idle": "2026-02-04T01:37:06.705220Z",
     "shell.execute_reply": "2026-02-04T01:37:06.704728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No seed binders; mutation skipped.\n"
     ]
    }
   ],
   "source": [
    "# Cell K: Iterative mutation optimization (Top10 -> Global Top100)\n",
    "MUT_DIR = OUTDIR / \"mutation\"\n",
    "MUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "# PyRosetta helpers\n",
    "\n",
    "def mutate_and_score(base_pdb, new_seq, receptor_chains=(\"A\",\"B\")):\n",
    "    ensure_pyrosetta()\n",
    "    import pyrosetta\n",
    "    from pyrosetta import rosetta\n",
    "\n",
    "    pose = pyrosetta.pose_from_pdb(base_pdb)\n",
    "    ensure_fullatom(pose)\n",
    "    binder_chain = infer_binder_chain_id(pose, receptor_chains)\n",
    "    if binder_chain is None:\n",
    "        return None\n",
    "    partners = get_partners(pose, binder_chain)\n",
    "\n",
    "    # Identify binder chain indices\n",
    "    chain_index = None\n",
    "    for i in range(1, pose.num_chains()+1):\n",
    "        ch = pose.pdb_info().chain(pose.chain_begin(i))\n",
    "        if ch == binder_chain:\n",
    "            chain_index = i\n",
    "            break\n",
    "    if chain_index is None:\n",
    "        return None\n",
    "    start = pose.chain_begin(chain_index)\n",
    "    end = pose.chain_end(chain_index)\n",
    "    if len(new_seq) != (end - start + 1):\n",
    "        return None\n",
    "\n",
    "    # Mutate residues\n",
    "    for i, aa in enumerate(new_seq):\n",
    "        resi = start + i\n",
    "        if pose.residue(resi).name1() != aa:\n",
    "            mut = rosetta.protocols.simple_moves.MutateResidue(resi, aa)\n",
    "            mut.apply(pose)\n",
    "\n",
    "    # Repack binder sidechains only\n",
    "    scorefxn = rosetta.core.scoring.get_score_function()\n",
    "    tf = rosetta.core.pack.task.TaskFactory()\n",
    "    tf.push_back(rosetta.core.pack.task.operation.InitializeFromCommandline())\n",
    "    tf.push_back(rosetta.core.pack.task.operation.RestrictToRepacking())\n",
    "    binder_sel = rosetta.core.select.residue_selector.ChainSelector(binder_chain)\n",
    "    not_binder = rosetta.core.select.residue_selector.NotResidueSelector(binder_sel)\n",
    "    tf.push_back(rosetta.core.pack.task.operation.OperateOnResidueSubset(\n",
    "        rosetta.core.pack.task.operation.PreventRepackingRLT(), not_binder))\n",
    "\n",
    "    packer = rosetta.protocols.minimization_packing.PackRotamersMover(scorefxn)\n",
    "    packer.task_factory(tf)\n",
    "    packer.apply(pose)\n",
    "\n",
    "    # Interface score\n",
    "    dG = interface_score_pyrosetta(pose, partners)\n",
    "    return dG\n",
    "\n",
    "# Seed sequences\n",
    "if CONFIG[\"dock_engine\"] != \"pyrosetta\" or seed_top.empty:\n",
    "    pd.DataFrame(columns=[\"sequence\",\"effective_score\",\"seed_id\",\"base_pose_path\"]).to_csv(MUT_DIR / \"top100_scores_fast.csv\", index=False)\n",
    "    (MUT_DIR / \"top100.fasta\").write_text(\"\")\n",
    "    pd.DataFrame(columns=[\"seed_id\",\"iter\",\"sequence\",\"docking_score\",\"effective_score\",\"accepted\",\"base_pose_path\"]).to_csv(MUT_DIR / \"mutation_trace.csv\", index=False)\n",
    "    print(\"No seed binders; mutation skipped.\")\n",
    "else:\n",
    "    seed_ids = seed_top[\"binder_id\"].tolist()\n",
    "    seed_scores = {row[\"binder_id\"]: row[\"docking_score\"] for _, row in seed_top.iterrows()}\n",
    "    seed_seq_map = {row[\"binder_id\"]: row.get(\"sequence\", \"\") for _, row in merged.iterrows()}\n",
    "    seed_pose_map = {row[\"binder_id\"]: row.get(\"best_pose_path\", \"\") for _, row in fast_df.iterrows()}\n",
    "\n",
    "    trace_rows = []\n",
    "    all_candidates = {}\n",
    "\n",
    "    for seed_id in seed_ids:\n",
    "        current_seq = seed_seq_map.get(seed_id, \"\")\n",
    "        base_pdb = seed_pose_map.get(seed_id)\n",
    "        if not current_seq or not base_pdb:\n",
    "            continue\n",
    "\n",
    "        # Base score from docking\n",
    "        base_score = seed_scores.get(seed_id, 0.0)\n",
    "\n",
    "        for it in range(CONFIG[\"iters\"]):\n",
    "            props = []\n",
    "            for _ in range(CONFIG[\"proposals_per_iter\"]*2):\n",
    "                i = random.randrange(len(current_seq))\n",
    "                aa = random.choice(amino_acids)\n",
    "                if aa == 'L':\n",
    "                    continue  # avoid introducing new Leu by default\n",
    "                new_seq = current_seq[:i] + aa + current_seq[i+1:]\n",
    "                props.append(new_seq)\n",
    "                if len(props) >= CONFIG[\"proposals_per_iter\"]:\n",
    "                    break\n",
    "\n",
    "            for pseq in props:\n",
    "                feats = seq_features(pseq)\n",
    "                if feats[\"count_L\"] > CONFIG[\"max_L\"] or feats[\"count_LL\"] > CONFIG[\"max_LL\"]:\n",
    "                    continue\n",
    "                dG = mutate_and_score(base_pdb, pseq)\n",
    "                if dG is None:\n",
    "                    continue\n",
    "                eff = dG + CONFIG[\"L_penalty_alpha\"] * max(0, feats[\"frac_L\"] - CONFIG[\"L_target\"])\n",
    "                trace_rows.append({\n",
    "                    \"seed_id\": seed_id,\n",
    "                    \"iter\": it,\n",
    "                    \"sequence\": pseq,\n",
    "                    \"docking_score\": dG,\n",
    "                    \"effective_score\": eff,\n",
    "                    \"accepted\": False,\n",
    "                    \"base_pose_path\": base_pdb,\n",
    "                })\n",
    "                if eff < base_score:\n",
    "                    current_seq = pseq\n",
    "                    base_score = eff\n",
    "                    trace_rows[-1][\"accepted\"] = True\n",
    "                    all_candidates[pseq] = (eff, seed_id, base_pdb)\n",
    "\n",
    "        # keep final sequence\n",
    "        all_candidates[current_seq] = (base_score, seed_id, base_pdb)\n",
    "\n",
    "    # Build Top100\n",
    "    cand_rows = []\n",
    "    for seq, (eff, seed_id, base_pdb) in all_candidates.items():\n",
    "        cand_rows.append({\"sequence\": seq, \"effective_score\": eff, \"seed_id\": seed_id, \"base_pose_path\": base_pdb})\n",
    "\n",
    "    if cand_rows:\n",
    "        cand_df = pd.DataFrame(cand_rows).sort_values(\"effective_score\").drop_duplicates(\"sequence\")\n",
    "    else:\n",
    "        cand_df = pd.DataFrame(columns=[\"sequence\",\"effective_score\",\"seed_id\",\"base_pose_path\"])\n",
    "\n",
    "    cand_df.head(CONFIG[\"keep_global\"]).to_csv(MUT_DIR / \"top100_scores_fast.csv\", index=False)\n",
    "\n",
    "    with open(MUT_DIR / \"top100.fasta\", \"w\") as f:\n",
    "        for i, row in cand_df.head(CONFIG[\"keep_global\"]).iterrows():\n",
    "            f.write(f\">mut_{i}\\n{row['sequence']}\\n\")\n",
    "\n",
    "    pd.DataFrame(trace_rows).to_csv(MUT_DIR / \"mutation_trace.csv\", index=False)\n",
    "    print(\"Mutation optimization completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b0ddf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:06.706281Z",
     "iopub.status.busy": "2026-02-04T01:37:06.706198Z",
     "iopub.status.idle": "2026-02-04T01:37:06.713250Z",
     "shell.execute_reply": "2026-02-04T01:37:06.712892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final docking fallback: using fast docking scores.\n"
     ]
    }
   ],
   "source": [
    "# Cell L: Top100 refined docking (high sampling)\n",
    "FINAL_DIR = OUTDIR / \"final\"\n",
    "FINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    mut_df = pd.read_csv(MUT_DIR / \"top100_scores_fast.csv\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    mut_df = pd.DataFrame(columns=[\"sequence\",\"effective_score\",\"seed_id\",\"base_pose_path\"])\n",
    "\n",
    "if mut_df.empty:\n",
    "    # Fallback: use fast docking scores if mutation/docking skipped\n",
    "    try:\n",
    "        fast_df = pd.read_csv(DOCK_DIR / \"docking_scores_fast.csv\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        fast_df = pd.DataFrame(columns=[\"binder_id\",\"docking_score\",\"best_pose_path\"])\n",
    "\n",
    "    if not fast_df.empty:\n",
    "        seq_df = sim_df[[\"binder_id\",\"sequence\"]].drop_duplicates()\n",
    "        final_df = fast_df.merge(seq_df, on=\"binder_id\", how=\"left\")\n",
    "        final_df = final_df.rename(columns={\"docking_score\": \"final_docking_score\"})\n",
    "        if \"effective_score\" not in final_df.columns:\n",
    "            final_df[\"effective_score\"] = np.nan\n",
    "        final_df.to_csv(FINAL_DIR / \"final_scores.csv\", index=False)\n",
    "        print(\"Final docking fallback: using fast docking scores.\")\n",
    "    else:\n",
    "        pd.DataFrame(columns=[\"binder_id\",\"final_docking_score\",\"best_pose_path\",\"sequence\",\"effective_score\"]).to_csv(FINAL_DIR / \"final_scores.csv\", index=False)\n",
    "        print(\"No candidates for final docking.\")\n",
    "else:\n",
    "    rows = []\n",
    "    if CONFIG[\"dock_engine\"] == \"contact_proxy\":\n",
    "        for idx, row in mut_df.head(CONFIG[\"max_final_dock\"]).iterrows():\n",
    "            base_pdb = row.get(\"base_pose_path\", \"\")\n",
    "            if not base_pdb or not os.path.exists(base_pdb):\n",
    "                continue\n",
    "            score = contact_score(base_pdb, cutoff=CONFIG[\"contact_cutoff\"])\n",
    "            rows.append({\n",
    "                \"binder_id\": f\"mut_{idx}\",\n",
    "                \"final_docking_score\": score,\n",
    "                \"best_pose_path\": base_pdb,\n",
    "                \"sequence\": row.get(\"sequence\", \"\"),\n",
    "                \"effective_score\": row.get(\"effective_score\", None),\n",
    "            })\n",
    "    else:\n",
    "        for idx, row in mut_df.head(CONFIG[\"max_final_dock\"]).iterrows():\n",
    "            seq = row.get(\"sequence\", \"\")\n",
    "            base_pdb = row.get(\"base_pose_path\", \"\")\n",
    "            if not base_pdb or not os.path.exists(base_pdb):\n",
    "                continue\n",
    "\n",
    "            ensure_pyrosetta()\n",
    "            import pyrosetta\n",
    "            pose = pyrosetta.pose_from_pdb(base_pdb)\n",
    "            ensure_fullatom(pose)\n",
    "            binder_chain = infer_binder_chain_id(pose)\n",
    "            if binder_chain is None:\n",
    "                continue\n",
    "            partners = get_partners(pose, binder_chain)\n",
    "            score, best_pose = dock_pose_pyrosetta(pose, partners, CONFIG[\"decoys_final\"])\n",
    "            best_pose_path = str(FINAL_DIR / f\"{idx}_best_final.pdb\")\n",
    "            if best_pose is not None:\n",
    "                best_pose.dump_pdb(best_pose_path)\n",
    "            rows.append({\n",
    "                \"binder_id\": f\"mut_{idx}\",\n",
    "                \"final_docking_score\": score,\n",
    "                \"best_pose_path\": best_pose_path,\n",
    "                \"sequence\": seq,\n",
    "                \"effective_score\": row.get(\"effective_score\", None),\n",
    "            })\n",
    "\n",
    "    final_df = pd.DataFrame(rows)\n",
    "    final_df.to_csv(FINAL_DIR / \"final_scores.csv\", index=False)\n",
    "    print(\"Final docking completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a40b042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:06.714321Z",
     "iopub.status.busy": "2026-02-04T01:37:06.714232Z",
     "iopub.status.idle": "2026-02-04T01:37:07.488900Z",
     "shell.execute_reply": "2026-02-04T01:37:07.488529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ pymol -cq /home/xuchengjie/Program/BindCraft/outputs/viz/top10_pymol/CDK4_CyclinD1_l50_s961754.pml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ pymol -cq /home/xuchengjie/Program/BindCraft/outputs/viz/top10_pymol/CDK4_CyclinD1_l50_s511797.pml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization assets prepared.\n"
     ]
    }
   ],
   "source": [
    "# Cell M: Top10 interaction visualization (PPI)\n",
    "VIZ_DIR = OUTDIR / \"viz\"\n",
    "(VIZ_DIR / \"top10_interactions\").mkdir(parents=True, exist_ok=True)\n",
    "(VIZ_DIR / \"top10_pymol\").mkdir(parents=True, exist_ok=True)\n",
    "(VIZ_DIR / \"top10_figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    final_df = pd.read_csv(FINAL_DIR / \"final_scores.csv\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    final_df = pd.DataFrame(columns=[\"binder_id\",\"best_pose_path\"])\n",
    "\n",
    "\n",
    "def compute_contacts_from_pdb(pdb_path, receptor_chains=(\"A\",\"B\"), cutoff=4.5):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"cplx\", pdb_path)\n",
    "    # Infer binder chain\n",
    "    binder_chain = None\n",
    "    for ch in structure[0]:\n",
    "        if ch.id not in receptor_chains:\n",
    "            binder_chain = ch.id\n",
    "            break\n",
    "    if binder_chain is None:\n",
    "        # fallback: shortest chain\n",
    "        chain_lens = {ch.id: sum(1 for r in ch if r.id[0]==\" \") for ch in structure[0]}\n",
    "        if chain_lens:\n",
    "            binder_chain = min(chain_lens, key=chain_lens.get)\n",
    "    if binder_chain is None:\n",
    "        return [], [], []\n",
    "\n",
    "    rec_atoms = []\n",
    "    for ch in receptor_chains:\n",
    "        if ch in structure[0]:\n",
    "            rec_atoms += [a for a in structure[0][ch].get_atoms() if a.get_parent().id[0] == \" \"]\n",
    "    lig_atoms = [a for a in structure[0][binder_chain].get_atoms() if a.get_parent().id[0] == \" \"]\n",
    "\n",
    "    ns = NeighborSearch(rec_atoms)\n",
    "    contacts = []\n",
    "    rec_res = set()\n",
    "    lig_res = set()\n",
    "    for a in lig_atoms:\n",
    "        close = ns.search(a.coord, cutoff)\n",
    "        for b in close:\n",
    "            ra = b.get_parent()\n",
    "            la = a.get_parent()\n",
    "            rec_res.add((ra.get_parent().id, ra.id[1]))\n",
    "            lig_res.add((la.get_parent().id, la.id[1]))\n",
    "            contacts.append((\n",
    "                (ra.get_parent().id, ra.id[1], b.get_name()),\n",
    "                (la.get_parent().id, la.id[1], a.get_name()),\n",
    "                float(a - b)\n",
    "            ))\n",
    "    # sort by distance\n",
    "    contacts = sorted(contacts, key=lambda x: x[2])\n",
    "    return sorted(rec_res), sorted(lig_res), contacts[:20]\n",
    "\n",
    "\n",
    "def write_pymol_script(pdb_path, out_pml, receptor_chains=(\"A\",\"B\"), contacts=None):\n",
    "    # Simple PyMOL script for interface visualization\n",
    "    script = [f\"load {pdb_path}\", \"remove solvent\", \"hide everything\"]\n",
    "    script.append(f\"select receptor, chain {'+'.join(receptor_chains)}\")\n",
    "    script.append(f\"select binder, not (chain {'+'.join(receptor_chains)})\")\n",
    "    script.append(\"show cartoon, receptor\")\n",
    "    script.append(\"show cartoon, binder\")\n",
    "    script.append(\"color cyan, receptor\")\n",
    "    script.append(\"color orange, binder\")\n",
    "    script.append(\"set cartoon_transparency, 0.2, receptor\")\n",
    "    script.append(\"set cartoon_transparency, 0.0, binder\")\n",
    "\n",
    "    # Draw distances for top contacts\n",
    "    if contacts:\n",
    "        for i, (rinfo, linfo, dist) in enumerate(contacts):\n",
    "            rch, rres, ratom = rinfo\n",
    "            lch, lres, latom = linfo\n",
    "            script.append(f\"distance d{i}, (chain {rch} and resi {rres} and name {ratom}), (chain {lch} and resi {lres} and name {latom})\")\n",
    "    script.append(f\"png {str(Path(out_pml).with_suffix('.png'))}, dpi=300, ray=1\")\n",
    "\n",
    "    Path(out_pml).write_text(\"\\n\".join(script) + \"\\n\")\n",
    "\n",
    "if final_df.empty:\n",
    "    print(\"No final docking results; skipping visualization.\")\n",
    "else:\n",
    "    # Make top10\n",
    "    for _, row in final_df.head(10).iterrows():\n",
    "        pdb_path = row.get(\"best_pose_path\")\n",
    "        if not pdb_path or not os.path.exists(str(pdb_path)):\n",
    "            continue\n",
    "        rec_res, lig_res, contacts = compute_contacts_from_pdb(pdb_path, cutoff=CONFIG[\"contact_cutoff\"])\n",
    "        out_json = VIZ_DIR / \"top10_interactions\" / f\"{row['binder_id']}.json\"\n",
    "        out_json.write_text(json.dumps({\n",
    "            \"receptor_residues\": rec_res,\n",
    "            \"binder_residues\": lig_res,\n",
    "            \"top_contacts\": contacts,\n",
    "            \"pdb_path\": pdb_path\n",
    "        }, indent=2))\n",
    "\n",
    "        # PyMOL script\n",
    "        out_pml = VIZ_DIR / \"top10_pymol\" / f\"{row['binder_id']}.pml\"\n",
    "        write_pymol_script(pdb_path, out_pml, contacts=contacts)\n",
    "\n",
    "        # Optional PNG via PyMOL\n",
    "        fig_path = VIZ_DIR / \"top10_figures\" / f\"{row['binder_id']}.png\"\n",
    "        if check_tool(CONFIG[\"pymol_exe\"]):\n",
    "            cmd = [CONFIG[\"pymol_exe\"], \"-cq\", str(out_pml)]\n",
    "            try:\n",
    "                run(cmd)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Fallback: simple contact-distance plot\n",
    "        if not fig_path.exists():\n",
    "            plt.figure(figsize=(4,3))\n",
    "            if contacts:\n",
    "                dists = [c[2] for c in contacts]\n",
    "                plt.plot(range(1, len(dists)+1), dists, marker='o')\n",
    "                plt.xlabel(\"Contact rank\")\n",
    "                plt.ylabel(\"Distance (A)\")\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, \"No contacts\", ha=\"center\", va=\"center\")\n",
    "                plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(fig_path, dpi=200)\n",
    "            plt.close()\n",
    "\n",
    "    print(\"Visualization assets prepared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d75563f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T01:37:07.489841Z",
     "iopub.status.busy": "2026-02-04T01:37:07.489757Z",
     "iopub.status.idle": "2026-02-04T01:37:07.496553Z",
     "shell.execute_reply": "2026-02-04T01:37:07.496206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   binder_id  final_docking_score  effective_score  \\\n",
      "0  CDK4_CyclinD1_l50_s961754               -905.0              NaN   \n",
      "1  CDK4_CyclinD1_l50_s511797               -834.0              NaN   \n",
      "\n",
      "                                            sequence  \n",
      "0  MQRVKEVRRQIRNYRETIDGIGIWMTRDKFMDRVWEIMVEVFTRME...  \n",
      "1  PKLSDFDRHMDRMRTPAQVRESNAHMSKYAKRHFTPKEVKQWEAPD...  \n",
      "\n",
      "Key outputs:\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/clean/6p8f_ABC.pdb\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/clean/6p8h_ABC.pdb\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/clean/2w9z_AB.pdb\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/hotspots/hotspots_union_2w9z.txt\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/bindcraft/designs.fasta\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/filter/similarity_metrics.csv\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/filter/kept.fasta\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/docking/docking_scores_fast.csv\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/analysis/seq_score_correlations.csv\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/mutation/mutation_trace.csv\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/mutation/top100.fasta\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/final/final_scores.csv\n",
      "- /home/xuchengjie/Program/BindCraft/outputs/REPORT.md\n"
     ]
    }
   ],
   "source": [
    "# Summary cell: Final report\n",
    "REPORT = OUTDIR / \"REPORT.md\"\n",
    "\n",
    "try:\n",
    "    final_df = pd.read_csv(FINAL_DIR / \"final_scores.csv\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    final_df = pd.DataFrame(columns=[\"binder_id\",\"final_docking_score\",\"effective_score\",\"sequence\"])\n",
    "\n",
    "# Top10 summary table\n",
    "summary_cols = [\"binder_id\", \"final_docking_score\", \"effective_score\", \"sequence\"]\n",
    "if not final_df.empty:\n",
    "    print(final_df.head(10)[summary_cols])\n",
    "else:\n",
    "    print(\"No final docking scores available.\")\n",
    "\n",
    "# Write report\n",
    "report_lines = []\n",
    "report_lines.append(\"# CDK4-CyclinD1 Binder Pipeline Report\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(\"## Parameters\")\n",
    "report_lines.append(\"```json\")\n",
    "report_lines.append(json.dumps(CONFIG, indent=2))\n",
    "report_lines.append(\"```\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(\"## Outputs\")\n",
    "report_lines.append(f\"- Clean PDBs: {CLEAN_DIR}\")\n",
    "report_lines.append(f\"- Hotspots: {HOTSPOT_DIR}\")\n",
    "report_lines.append(f\"- BindCraft: {BINDCRAFT_DIR}\")\n",
    "report_lines.append(f\"- Filter: {FILTER_DIR}\")\n",
    "report_lines.append(f\"- Docking: {DOCK_DIR}\")\n",
    "report_lines.append(f\"- Analysis: {ANALYSIS_DIR}\")\n",
    "report_lines.append(f\"- Mutation: {MUT_DIR}\")\n",
    "report_lines.append(f\"- Final: {FINAL_DIR}\")\n",
    "report_lines.append(f\"- Viz: {VIZ_DIR}\")\n",
    "\n",
    "notes = []\n",
    "if \"smoke_2stage\" in str(CONFIG.get(\"bindcraft_advanced\", \"\")):\n",
    "    notes.append(\"- BindCraft used smoke_2stage settings for resource-limited execution.\")\n",
    "if CONFIG.get(\"bindcraft_trim_on_oom\"):\n",
    "    notes.append(f\"- Target trimmed around hotspots (radius={CONFIG.get('trim_radius')}) on OOM fallback.\")\n",
    "if CONFIG.get(\"dock_engine\") == \"contact_proxy\":\n",
    "    notes.append(\"- Docking used contact-count proxy scoring (PyRosetta disabled due to stability).\")\n",
    "if notes:\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"## Notes\")\n",
    "    report_lines.extend(notes)\n",
    "\n",
    "if final_df.empty:\n",
    "    report_lines.append(\"\")\n",
    "    report_lines.append(\"## Results\")\n",
    "    report_lines.append(\"- No final docking results were produced.\")\n",
    "\n",
    "REPORT.write_text(\"\\n\".join(report_lines))\n",
    "\n",
    "# Print output tree (key files)\n",
    "key_files = [\n",
    "    CLEAN_DIR / \"6p8f_ABC.pdb\",\n",
    "    CLEAN_DIR / \"6p8h_ABC.pdb\",\n",
    "    CLEAN_DIR / \"2w9z_AB.pdb\",\n",
    "    HOTSPOT_DIR / \"hotspots_union_2w9z.txt\",\n",
    "    BINDCRAFT_DIR / \"designs.fasta\",\n",
    "    FILTER_DIR / \"similarity_metrics.csv\",\n",
    "    FILTER_DIR / \"kept.fasta\",\n",
    "    DOCK_DIR / \"docking_scores_fast.csv\",\n",
    "    ANALYSIS_DIR / \"seq_score_correlations.csv\",\n",
    "    MUT_DIR / \"mutation_trace.csv\",\n",
    "    MUT_DIR / \"top100.fasta\",\n",
    "    FINAL_DIR / \"final_scores.csv\",\n",
    "    REPORT,\n",
    "]\n",
    "print(\"\\nKey outputs:\")\n",
    "for p in key_files:\n",
    "    print(\"-\", p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
